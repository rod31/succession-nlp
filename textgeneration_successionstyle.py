# -*- coding: utf-8 -*-
"""TextGeneration_SuccessionStyle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ifRl6xZPeM9KPlzLNmZDgokGC7-4KKWZ
"""

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

import os
print(os.listdir('/content/drive/My Drive/Data Science Projects/RoyGPT'))
roy_gpt_path = '/content/drive/My Drive/Data Science Projects/RoyGPT'

"""Defining file paths"""

season_one_path = os.path.join(roy_gpt_path, 'Succession_Season_One.pdf')
season_two_path = os.path.join(roy_gpt_path, 'Succession_Season_Two.pdf')
season_three_path = os.path.join(roy_gpt_path, 'Succession_Season_Three.pdf')
season_four_path = os.path.join(roy_gpt_path, 'Succession_Season_Four.pdf')

"""**pdfplumber** is effective at extracting text from PDFs, especially when the layout is complex."""

# !pip install pdfplumber==0.11.3

"""This function takes the path to a PDF file (pdf_path) as input and **returns all the text content of the PDF** as a single string."""

# import pdfplumber

# def extract_text_from_pdf(pdf_path):
#     text = ""
#     with pdfplumber.open(pdf_path) as pdf:
#         for page in pdf.pages:
#             text += page.extract_text()
#     return text

# text_season_one = extract_text_from_pdf(season_one_path)
# text_season_two = extract_text_from_pdf(season_two_path)
# text_season_three = extract_text_from_pdf(season_three_path)
# text_season_four = extract_text_from_pdf(season_four_path)

# # Build a dictionary
# seasons_dict = {
#     'Season 1': text_season_one,
#     'Season 2': text_season_two,
#     'Season 3': text_season_three,
#     'Season 4': text_season_fourA
# }
# # Print the first 500 characters of the text from the first PDF as an example
# print(list(seasons_dict.values())[0][:500])

"""Attempt to trim text: **FAILED**"""

# import re

# def trim_script(text):
#     # Look for the first occurrence of the word 'EPISODE' in a case-sensitive manner
#     text = text.strip()
#     match = re.search(r'\bEPISODE\b', text)

#     if match:
#         # Return the text starting from the first occurrence of 'EPISODE'
#         return text[match.start():]

#     # If 'EPISODE' is not found, return the original text
#     return text

# # Apply the trimming function to each season's text
# for season in seasons_dict:
#     seasons_dict[season] = trim_script(seasons_dict[season])

# # Print the first 500 characters of the trimmed text to verify
# print(list(seasons_dict.values())[0][:500])

"""Extracting Roman Roy's Dialogues: FAILED"""

# import re

# def extract_roman_dialogues(season_text):
#     roman_dialogues = []
#     # Regex pattern to match Roman's name and capture his dialogues
#     pattern = r'\bROMAN\b\s*(.*?)(?=\n[A-Z ]+\n|\n\n|\Z)'
#     matches = re.findall(pattern, season_text, re.DOTALL)

#     for match in matches:
#         # Clean up the dialogue, removing stage directions in parentheses
#         dialogue = re.sub(r'\(.*?\)', '', match).strip()
#         # Append non-empty dialogue
#         if dialogue:
#             roman_dialogues.append(dialogue)

#     return roman_dialogues

# # Apply the function to extract Roman's dialogues
# roman_dialogues_season_one = extract_roman_dialogues(text_season_one)

# # Print the first few dialogues to verify
# print(roman_dialogues_season_one[:10])

"""^Issues Identified:
Non-dialogue Text Still Present:

Phrases like "ROY\nAlan Ruck" and stage directions such as "The business alchemist starts to pack up his kit." and "Logan walks the unfamiliar rooms of his large new apartment..." are incorrectly included in the output.
Combining Multiple Dialogue Blocks:

The extraction sometimes combines Roman's dialogue with other character’s actions or stage directions, making the output cluttered and less useful.

"""

# import re

# def extract_roman_dialogues(season_text):
#     roman_dialogues = []
#     # Regex pattern to match Roman's name and capture his dialogues
#     pattern = r'\bROMAN\b\s*\n(.*?)(?=\n[A-Z ]+\n|\n{2,}|\Z)'
#     matches = re.findall(pattern, season_text, re.DOTALL)

#     for match in matches:
#         # Filter out known non-dialogue text (e.g., cast lists)
#         if "Alan Ruck" in match or re.match(r'^\s*$', match):
#             continue
#         # Clean up the dialogue, removing stage directions and extra text
#         cleaned_dialogue = re.sub(r'\(.*?\)', '', match)  # Remove anything in parentheses (stage directions)
#         cleaned_dialogue = re.sub(r'\n+', ' ', cleaned_dialogue)  # Replace newlines within the dialogue with spaces
#         cleaned_dialogue = cleaned_dialogue.strip()  # Remove leading and trailing whitespace

#         # Further cleanup to remove any potential remnants of italic indicators if they exist (heuristic approach)
#         # Assuming no special characters for italics, but just spaces or odd formatting, we handle it by a regex like:
#         cleaned_dialogue = re.sub(r'\*.*?\*', '', cleaned_dialogue)  # Example of removing something in stars or similar

#         # Filter out any line that consists of just uppercase words (often non-dialogue text)
#         if re.match(r'^[A-Z ]+$', cleaned_dialogue):
#             continue
#         if cleaned_dialogue:
#             roman_dialogues.append(cleaned_dialogue)

#     return roman_dialogues

# # Apply the function to extract Roman's dialogues
# roman_dialogues_season_one = extract_roman_dialogues(text_season_one)

# # Print the first few dialogues to verify
# print(roman_dialogues_season_one[:10])

"""^ISSUE: still includes scene descriptions in the dialogue.

IDEA: remove italicized text (used a lot for scene descriptions in this PDF)

OPTION: Using PyMuPDF (fitz)
PyMuPDF is another Python library that can be used to extract text while retaining some formatting information, including whether text is italicized. This library can extract text along with font information, which can be used to detect italics.
"""

!pip install PyMuPDF

"""**Extract Text with Italics**
All dialogues in this script are italicized.
"""

import fitz  # PyMuPDF

def extract_text_with_italics(pdf_path):
    doc = fitz.open(pdf_path)
    text_blocks = []

    for page in doc:
        blocks = page.get_text("dict")["blocks"]
        for block in blocks:
            # Check if 'lines' key exists in the block
            if "lines" in block:
                for line in block["lines"]:
                    line_text = ""
                    for span in line["spans"]:
                        # Check if the font style indicates italics (could vary by PDF)
                        if "italic" in span["font"].lower():
                            line_text += f"*{span['text']}*"  # Use *text* to indicate italics
                        else:
                            line_text += span["text"]
                    text_blocks.append(line_text)

    return "\n".join(text_blocks)

"""**Extracting dialogues for each season**"""

text_season_one = extract_text_with_italics(season_one_path)
text_season_two = extract_text_with_italics(season_two_path)
text_season_three = extract_text_with_italics(season_three_path)
text_season_four = extract_text_with_italics(season_four_path)

# Build a dictionary
seasons_dict = {
    'Season 1': text_season_one,
    'Season 2': text_season_two,
    'Season 3': text_season_three,
    'Season 4': text_season_four
}
# Print the first 500 characters of the text from the first PDF as an example
print(list(seasons_dict.values())[0][:500])

"""**Extracting Roman Roy's dialogues with heuristics**"""

import re

def extract_roman_dialogues(text_with_italics):
    roman_dialogues = []
    # Adjusted regex pattern to handle potential special cases more robustly
    pattern = r'\bROMAN\b\s*(.*?)(?=\n[A-Z ]+\n|\n{2,}|\Z)'
    matches = re.findall(pattern, text_with_italics, re.DOTALL)

    for match in matches:
        # Remove stage directions (marked with asterisks for italics)
        cleaned_dialogue = re.sub(r'\*.*?\*', '', match)  # Remove italicized text within asterisks
        cleaned_dialogue = re.sub(r'\s*\(\s*\)', '', cleaned_dialogue)  # Remove empty parentheses
        cleaned_dialogue = re.sub(r'\n+', ' ', cleaned_dialogue)  # Replace newlines within the dialogue with spaces
        cleaned_dialogue = re.sub(r'\*\s*$', '', cleaned_dialogue)  # Remove any trailing asterisks
        cleaned_dialogue = cleaned_dialogue.strip()  # Remove leading and trailing whitespace

        # Filter out non-dialogue lines (e.g., scene headings like INT. LOGAN'S HOME, or erroneous captures)
        if re.match(r'^[A-Z ]+$', cleaned_dialogue) or "INT." in cleaned_dialogue or "EXT." in cleaned_dialogue or "’S" in cleaned_dialogue:
            continue
        if cleaned_dialogue:
            roman_dialogues.append(cleaned_dialogue)

    return roman_dialogues

"""**Key Improvements:**

*Stage Directions Removed*: The italicized text that was previously marked by asterisks, such as stage directions, is no longer present in the dialogue.

*Clean Dialogues:* The dialogues are clean, with no extraneous formatting or content, except for one small instance of leftover stage direction at the end of the last line.
"""

# Initialize an empty dictionary to hold Roman's dialogues for each season
roman_dialogues_dict = {}

# Loop through each season's text in the dictionary
for season, text in seasons_dict.items():
    # Extract Roman's dialogues from the current season's text
    roman_dialogues = extract_roman_dialogues(text)
    # Store the extracted dialogues in the dictionary
    roman_dialogues_dict[season] = roman_dialogues

# Print out the first few dialogues from each season to verify
for season, dialogues in roman_dialogues_dict.items():
    print(f"{season}: {dialogues[:3]}")  # Print the first 3 dialogues as a sample

roman_dialogues_dict

"""**Tokenization and Text Cleaning:**
Prepare the text for any further analysis, model training, or other NLP tasks by breaking it into individual tokens (words, phrases) and performing additional cleaning if needed.
"""

!pip install editdistance

import nltk
from nltk.tokenize import word_tokenize
import re
import editdistance

# Ensure necessary NLTK data is available
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

def tokenize_and_clean(dialogues):
    stop_words = set(stopwords.words('english'))  # Get the list of stopwords
    all_tokens = []
    for dialogue in dialogues:
        # Remove any remaining special characters
        cleaned_dialogue = re.sub(r'[^\w\s]', '', dialogue)  # Remove punctuation, keep words and spaces
        # Tokenize the dialogue
        tokens = word_tokenize(cleaned_dialogue.lower())  # Tokenize and convert to lowercase

        # Handle misspelled stopwords using fuzzy matching
        cleaned_tokens = []
        for token in tokens:
            if not any(editdistance.eval(token, stop_word) <= 2 for stop_word in stop_words):
                cleaned_tokens.append(token)

        all_tokens.extend(cleaned_tokens)

    return all_tokens

# Apply tokenization and cleaning to each season
tokenized_dialogues = {}
for season, dialogues in roman_dialogues_dict.items():
    tokenized_dialogues[season] = tokenize_and_clean(dialogues)

"""***Exploratory Data Analysis (EDA):***

Once we have tokenized the text, we can perform some exploratory data analysis to understand Roman's dialogues better.
"""

tokenized_dialogues

!pip install wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# Example: Word frequency analysis for each season
for season, tokens in tokenized_dialogues.items():
    word_freq = Counter(tokens)
    print(f"Most common words in {season}: {word_freq.most_common(10)}")
# Generate and display word clouds for each season
for season, tokens in tokenized_dialogues.items():
    if tokens:  # Check if the season has any tokens
        # Join tokens into a single string
        text = ' '.join(tokens)

        # Generate the word cloud
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

        # Display the word cloud
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'ROMAN\'s Word Cloud for {season}')
        plt.show()
    else:
        print(f"No dialogues found for {season}, skipping word cloud.")

# from nltk.util import ngrams
# from collections import Counter

# def get_ngrams(tokens, n=2):
#     n_grams = ngrams(tokens, n)
#     return Counter(n_grams)

# # Example: Bigram analysis for each season
# for season, tokens in tokenized_dialogues.items():
#     bigrams = get_ngrams(tokens, n=2)
#     print(f"Most common bigrams in {season}: {bigrams.most_common(10)}")

# import nltk
# nltk.download('vader_lexicon')

# from nltk.sentiment import SentimentIntensityAnalyzer

# def analyze_sentiment(dialogues):
#     sia = SentimentIntensityAnalyzer()
#     sentiment_scores = [sia.polarity_scores(dialogue) for dialogue in dialogues]
#     return sentiment_scores

# # Example: Sentiment analysis for each season
# for season, dialogues in roman_dialogues_dict.items():
#     if dialogues:  # Check if the season has any dialogues
#         sentiments = analyze_sentiment(dialogues)
#         avg_sentiment = sum(d['compound'] for d in sentiments) / len(sentiments)
#         print(f"Average sentiment in {season}: {avg_sentiment}")
#     else:
#         print(f"No dialogues found for {season}, skipping sentiment analysis.")

"""**Results above:** Roman was overall neutral in his language. However, we see a slight increase in positivity from season 1 to 3, and then a decrease in season 4. Does that reflect his feelings throughout the progress of the show?"""

# def lexical_diversity(tokens):
#     if len(tokens) == 0:  # Check if the token list is empty
#         return 0  # Return 0 or another appropriate value for empty lists
#     return len(set(tokens)) / len(tokens)

# # Example: Lexical diversity for each season
# for season, tokens in tokenized_dialogues.items():
#     if len(tokens) > 0:  # Ensure tokens list is not empty
#         diversity = lexical_diversity(tokens)
#         print(f"Lexical diversity in {season}: {diversity}")
#     else:
#         print(f"No dialogues found for {season}, skipping lexical diversity calculation.")

"""The drop in lexical diversity in Season 4 suggests that Roman's dialogue in this season has become slightly more repetitive compared to the earlier seasons.
A diversity score of 0.5203 is still relatively high, but the decrease compared to Seasons 1 and 3 could indicate that the scriptwriters used more recurring phrases or that Roman's character is focusing on specific themes or ideas more consistently in this season.
The slight drop in lexical diversity might indicate a shift in his character or the situations he finds himself in, where his language becomes more focused or repetitive, perhaps due to increased stress, responsibility, or consistent themes.
"""

def count_swear_words(tokens):
    swear_words = {'fuck', 'fucking', 'fuckin', 'shit', 'damn', 'asshole', 'bitch', 'dick', 'cock', 'cunt', 'pussy', 'motherfucker'}
    return sum(1 for token in tokens if token in swear_words)

# Example: Swear word count for each season
for season, tokens in tokenized_dialogues.items():
    swear_count = count_swear_words(tokens)
    print(f"Swear word count in {season}: {swear_count}")

# import matplotlib.pyplot as plt

# def dialogue_length_distribution(dialogues):
#     lengths = [len(word_tokenize(dialogue)) for dialogue in dialogues]
#     plt.hist(lengths, bins=20, alpha=0.75)
#     plt.xlabel('Dialogue Length (words)')
#     plt.ylabel('Frequency')
#     plt.title('Distribution of Dialogue Lengths')
#     plt.show()

# # Example: Dialogue length distribution for each season
# for season, dialogues in roman_dialogues_dict.items():
#     print(f"Dialogue length distribution in {season}:")
#     dialogue_length_distribution(dialogues)

"""^Not sure how much this tells us about Roman Roy's personality...

-------------------------------------------
**REVIEW:** Code below is definitely faulty. I added 'ken' and 'dad' to the characters' list since the count for Kendall and Logan was so strangely small.

I think the count for dialogues should take in consideration the character who spoke right before and/or right after Roman.

Current implementation seems to only detect if Roman explicitly says the name of another character.
"""

def character_interaction_analysis(dialogues, character_list):
    interaction_count = {character: 0 for character in character_list}
    for dialogue in dialogues:
        for character in character_list:
            if character.lower() in dialogue.lower():
                interaction_count[character] += 1

    # Sort the interaction_count dictionary by the counts in descending order
    sorted_interactions = dict(sorted(interaction_count.items(), key=lambda item: item[1], reverse=True))
    return sorted_interactions

# Example: Character interaction analysis
character_list = ['kendall', 'ken', 'dad', 'logan', 'shiv', 'siobhan', 'tom', 'greg', 'connor']
for season, dialogues in roman_dialogues_dict.items():
    interactions = character_interaction_analysis(dialogues, character_list)
    print(f"Character interactions in {season}: {interactions}")

"""**(UPDATE ANALYSIS)** The interaction analysis shows that Roman's character increasingly revolves around his relationships with Shiv and Tom, especially in the later seasons. This shift in focus could be driven by changes in the family's power dynamics, Roman's growing influence, or evolving alliances within the Roy family. Meanwhile, Kendall and Logan's reduced presence in Roman's interactions suggests a potential distancing or change in the nature of their relationships.

**Why are interactions with Logan and Kendall so low? Is this accurate?**

-------------------------------------------
**Sarcasm Detection:**

(I don't have labeled data for sarcasm, manually labeling the dataset would take too long. It would be cool, but difficult, to crowdsource the labeling as an online game. "Determine if the following dialogue from *Succession* contains sarcasm or not")

ANOTHER OPTION: Transfer Learning
"""

!pip install tensorflow

"""Export dialogues for manual labeling (using Google Sheets)"""

import pandas as pd

# Create a DataFrame with the text and an empty "Is_Sarcastic" column
df = pd.DataFrame({'text': roman_dialogues, 'Is_Sarcastic': None})  # None means unlabelled for now

# Save to CSV (you can specify a path where you'd like to save it)
df.to_csv('roman_roy_sarcasm_labels.csv', index=False)
print("CSV file saved!")

!pip install --upgrade gspread
from google.colab import auth
auth.authenticate_user()

"""Read Labeled dataset for training"""

import gspread
import pandas as pd
from google.auth import default

# Authenticate and access Google Sheets
creds, _ = default()
gc = gspread.authorize(creds)

# Open the Google Sheet by name
sheet = gc.open('roman_roy_sarcasm_labeled').sheet1  # Adjust if you have more than one sheet

# Get all values from the sheet
rows = sheet.get_all_values()

# Convert the data to a DataFrame
labeled_df = pd.DataFrame(rows[1:], columns=rows[0])  # Assuming the first row is the header

# Display the DataFrame
print(labeled_df.head())

"""Try different tokenization for training the model."""

import re
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords

def tokenize(dialogues):
    stop_words = set(stopwords.words('english'))  # Get the list of stopwords
    all_tokens = []
    for dialogue in dialogues:
        # Remove special characters but keep common punctuation for sarcasm detection (like question marks, exclamations)
        cleaned_dialogue = re.sub(r'[^\w\s\?]', '', dialogue)  # Remove punctuation except ?, !

        # Tokenize the dialogue
        tokens = word_tokenize(cleaned_dialogue.lower())  # Tokenize and convert to lowercase

        # Only remove truly unimportant stopwords, keep negations like "not" or "no"
        cleaned_tokens = []
        for token in tokens:
            if token not in stop_words or token in ['no', 'not', 'isn’t', 'don’t']:  # Keep negations and crucial words
                cleaned_tokens.append(token)

        all_tokens.extend(cleaned_tokens)

    return all_tokens

import nltk
nltk.download('punkt_tab')
# Tokenize and clean all dialogues
labeled_df['cleaned_text'] = labeled_df['text'].apply(lambda x: ' '.join(tokenize([x])))

# Check the cleaned text
print(labeled_df[['text', 'cleaned_text']].head())

""" **Filter Out Rows with Missing Labels**:
 We can filter out the rows that have missing values in the "Is_Sarcastic" column so that you only use the labeled data for training.
"""

import numpy as np

# Replace empty strings with NaN, then fill NaN with 0, and convert to integers
labeled_df['Is_Sarcastic'] = labeled_df['Is_Sarcastic'].replace('', np.nan).fillna(0).astype(int)

# Proceed with the rest of the process
X = labeled_df['cleaned_text']
y = labeled_df['Is_Sarcastic']

from sklearn.model_selection import train_test_split

# Use the cleaned text for training
X = labeled_df['cleaned_text']
y = labeled_df['Is_Sarcastic'].astype(int)  # Ensure labels are integers

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the size of the splits
print(f"Training set: {len(X_train)} samples")
print(f"Test set: {len(X_test)} samples")

"""**Converting Text to Sequences of Integers for the Model**

Deep learning models require numeric input
"""

# Build a vocabulary manually from your tokenized training data
vocab = set([word for dialogue in X_train for word in dialogue.split()])  # Create vocabulary from training set
word_index = {word: i+1 for i, word in enumerate(vocab)}  # Assign a unique index to each word

# Convert each dialogue into a sequence of integers based on the word index
def text_to_sequence(dialogue, word_index):
    return [word_index.get(word, 0) for word in dialogue.split()]

# Convert training and testing data to sequences
X_train_sequences = [text_to_sequence(dialogue, word_index) for dialogue in X_train]
X_test_sequences = [text_to_sequence(dialogue, word_index) for dialogue in X_test]

# Pad the sequences to ensure they are of the same length
from tensorflow.keras.preprocessing.sequence import pad_sequences

max_length = 100  # Adjust as needed for your data
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')

# Check padded sequences
print(X_train_padded.shape)
print(X_test_padded.shape)

"""**Training the Model**

**Embedding Layer for Word Vectors**
 map each integer (representing a word) to a dense vector of real numbers. These embeddings **capture semantic relationships** between words, so, for instance, similar words (e.g., "happy" and "joyful") have similar vector representations.
"""

import tensorflow as tf

# Build a Neural Network model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=len(word_index) + 1, output_dim=16, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
num_epochs = 10
history = model.fit(
    X_train_padded,
    y_train,
    epochs=num_epochs,
    validation_data=(X_test_padded, y_test),
    verbose=2
)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

# Predict sarcasm in new dialogues
new_dialogues = ["Oh great, I am going to kill myself.", "‘Hey, Doctor Sarcasmo, did we ask you to squeak? Stand easy, pitch wall."]
new_dialogues_tokenized = [' '.join(tokenize([dialogue])) for dialogue in new_dialogues]  # use function tokenize() OR tokenize_and_clean()
new_sequences = [text_to_sequence(dialogue, word_index) for dialogue in new_dialogues_tokenized]
new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')

predictions = model.predict(new_padded)
sarcasm_labels = ['Sarcasm' if pred > 0.5 else 'Not Sarcasm' for pred in predictions]

# Display predictions
for dialogue, label in zip(new_dialogues, sarcasm_labels):
    print(f"Dialogue: {dialogue}\nPrediction: {label}")

"""**Try to fine-tune a Pre-Trained Transformer Model**

Pre-trained transformer models (like BERT) already have a rich understanding of language, as they have been trained on large corpora. We can fine-tune them on our specific sarcasm dataset, which means the model can start with a strong ability to understand language and then specialize in sarcasm detection.
"""

# Install the Transformers library
!pip install transformers
!pip install --upgrade tensorflow keras

"""**Load BERT and Tokenizer**"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification

# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Load the BERT model for binary classification (sarcasm vs. not sarcasm)
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

"""**Tokenize Your Dataset for BERT**"""

# Tokenize the dataset
def tokenize_data(texts, tokenizer, max_length=100):
    return tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="tf"
    )

# Tokenize the training and testing data
train_encodings = tokenize_data(X_train.tolist(), tokenizer)
test_encodings = tokenize_data(X_test.tolist(), tokenizer)

"""Convert the labels into TensorFlow tensors and prepare the data for training."""

# Convert labels to tensors
train_labels = tf.convert_to_tensor(y_train.tolist())
test_labels = tf.convert_to_tensor(y_test.tolist())

"""**Compile and Fine-Tune the BERT Model**


"""

from tensorflow.keras.optimizers import Adam
# Compile the model with an optimizer and loss function
model.compile(
    # optimizer=Adam(learning_rate=2e-5),
    optimizer='adam', # Use the string identifier 'adam'
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Fine-tune the model
history = model.fit(
    train_encodings['input_ids'],  # Tokenized text data
    train_labels,                  # Corresponding labels
    validation_data=(test_encodings['input_ids'], test_labels),
    epochs=3,                       # Adjust the number of epochs as needed
    batch_size=16                   # Adjust batch size based on available memory
)

"""Evaluate the Model"""

# Evaluate the model on the test set
loss, accuracy = model.evaluate(test_encodings['input_ids'], test_labels)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

"""Make predictions on new data"""

# Function to predict sarcasm
def predict_sarcasm(dialogues):
    encodings = tokenize_data(dialogues, tokenizer)
    predictions = model.predict(encodings['input_ids']).logits
    predicted_labels = tf.argmax(predictions, axis=1).numpy()
    return ["Sarcasm" if label == 1 else "Not Sarcasm" for label in predicted_labels]

# Test with new examples
new_dialogues = ["Oh great, I love doing all the paperwork without getting paid!", "What an amazing day!"]
predictions = predict_sarcasm(new_dialogues)
print(predictions)

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""**Confusion Matrix:**"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict on the test set
y_pred = model.predict(test_encodings['input_ids'])
y_pred_classes = tf.argmax(y_pred.logits, axis=1).numpy()

# Confusion matrix
cm = confusion_matrix(test_labels, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Sarcasm', 'Sarcasm'], yticklabels=['Not Sarcasm', 'Sarcasm'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

print(classification_report(test_labels, y_pred_classes, target_names=['Not Sarcasm', 'Sarcasm']))

"""Accuracy is 80% but precision for the positive class is 0, hence, the model is not detecting any Sarcasm.


P.S.: Accuracy can be misleading

---


IDEA: **Class Imbalance Techniques** (if applicable)
If your dataset is imbalanced (e.g., more non-sarcastic than sarcastic dialogues), consider applying  this technique to handle the imbalance:

Class weights: You can adjust the weights of the classes to give more importance to sarcastic examples. This approach will tell the model to give more importance to sarcastic examples, which should improve the model’s performance on the minority class.
"""

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Assuming `y_train` contains the labels (0 for non-sarcastic, 1 for sarcastic)
class_weights = compute_class_weight(
    class_weight='balanced',  # This will automatically compute the weights
    classes=np.unique(y_train),  # The two classes: 0 (non-sarcastic), 1 (sarcastic)
    y=y_train                  # The labels from the training data
)

# Convert the result into a dictionary
class_weights = dict(enumerate(class_weights))
print("Class Weights:", class_weights)

# Compile the model with an optimizer and loss function
model.compile(
    # optimizer=Adam(learning_rate=2e-5),
    optimizer='adam', # Use the string identifier 'adam'
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Train the model with class weights
history = model.fit(
    train_encodings['input_ids'],
    train_labels,
    validation_data=(test_encodings['input_ids'], test_labels),
    epochs=3,
    batch_size=16,
    class_weight=class_weights  # Apply the class weights here
)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(test_encodings['input_ids'], test_labels)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

# Function to predict sarcasm
def predict_sarcasm(dialogues):
    encodings = tokenize_data(dialogues, tokenizer)
    predictions = model.predict(encodings['input_ids']).logits
    predicted_labels = tf.argmax(predictions, axis=1).numpy()
    return ["Sarcasm" if label == 1 else "Not Sarcasm" for label in predicted_labels]

# Test with new examples
new_dialogues = ["Oh great, I love doing all the paperwork without getting paid!", "What an amazing day!"]
predictions = predict_sarcasm(new_dialogues)
print(predictions)

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict on the test set
y_pred = model.predict(test_encodings['input_ids'])
y_pred_classes = tf.argmax(y_pred.logits, axis=1).numpy()

# Confusion matrix
cm = confusion_matrix(test_labels, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Sarcasm', 'Sarcasm'], yticklabels=['Not Sarcasm', 'Sarcasm'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

print(classification_report(test_labels, y_pred_classes, target_names=['Not Sarcasm', 'Sarcasm']))

"""Let's try **Oversampling** now"""

from imblearn.over_sampling import RandomOverSampler

# Apply random oversampling to increase sarcastic examples
oversampler = RandomOverSampler(sampling_strategy='minority')  # Oversample sarcasm class
X_train_resampled, y_train_resampled = oversampler.fit_resample(train_encodings['input_ids'], y_train)

model.compile(
    # optimizer=Adam(learning_rate=2e-5),
    optimizer='adam', # Use the string identifier 'adam'
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

# Train the model with the new balanced dataset
history = model.fit(
    X_train_resampled,
    y_train_resampled,
    validation_data=(test_encodings['input_ids'], test_labels),
    epochs=3,
    batch_size=16
)
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Get predictions
y_pred = model.predict(test_encodings['input_ids']).logits
y_pred_classes = tf.argmax(y_pred, axis=1).numpy()

# Print classification report
print(classification_report(test_labels, y_pred_classes, target_names=['Not Sarcasm', 'Sarcasm']))

# Plot confusion matrix
cm = confusion_matrix(test_labels, y_pred_classes)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Sarcasm', 'Sarcasm'], yticklabels=['Not Sarcasm', 'Sarcasm'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""

**Possible Issues and Explanations**:
*The Model Isn’t Learning Sarcasm Properly*

- BERT is pre-trained for general language understanding, NOT sarcasm detection.

- It doesn’t inherently understand sarcasm unless fine-tuned well on a large sarcasm-specific dataset. My dataset may not have enough labeled sarcastic examples for effective fine-tuning.
- Sarcasm often depends on context, tone, and prior knowledge, making it hard to detect with just text-based models. (should I train the model with the dialogues of characters to whom Roman is talking/responding?)

**IDEA:**
Instead of just fine-tuning on your dataset, you could first pretrain the model on a larger sarcasm dataset (e.g., the Kaggle Sarcasm Dataset), THEN fine-tune on Roman Roy’s dialogues."""

